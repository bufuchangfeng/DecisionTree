{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import Counter\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    data = loadmat(\"data/mnist_all.mat\")\n",
    "\n",
    "    # print(data.keys())\n",
    "\n",
    "    train_data = pd.DataFrame()\n",
    "    test_data = pd.DataFrame()\n",
    "\n",
    "    for i in range(10):\n",
    "        temp_df = pd.DataFrame(data[\"train\" + str(i)])\n",
    "        temp_df['label'] = i\n",
    "        train_data = train_data.append(temp_df)\n",
    "        temp_df = pd.DataFrame(data[\"test\" + str(i)])\n",
    "        temp_df['label'] = i\n",
    "        test_data = test_data.append(temp_df)\n",
    "\n",
    "    train_data = shuffle(train_data)\n",
    "    test_data = shuffle(test_data)\n",
    "\n",
    "    train_labels = np.array(train_data['label'])\n",
    "    test_labels = np.array(test_data['label'])\n",
    "\n",
    "    train_data = train_data.drop('label', axis=1)\n",
    "    test_data = test_data.drop('label', axis=1)\n",
    "    \n",
    "    train_data = np.array(train_data) / 255\n",
    "    test_data = np.array(test_data) / 255\n",
    "    \n",
    "    pca = PCA(0.95)\n",
    "    pca.fit(train_data)\n",
    "    train_data = pca.transform(train_data)\n",
    "    test_data = pca.transform(test_data)\n",
    "\n",
    "    return train_data, test_data, train_labels, test_labels\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_data()\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意，经过PCA降维后，认为所有的特征都是连续值\n",
    "X = np.concatenate((X_train, X_valid), axis=0)\n",
    "y = np.concatenate((y_train, y_valid), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 154) (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=2, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "standard_xgboost = XGBClassifier(n_estimators=2, learning_rate=0.1, max_depth=3)\n",
    "standard_xgboost.fit(X[:100], y[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard xgboost test acc: 0.4237\n"
     ]
    }
   ],
   "source": [
    "predict_standard = standard_xgboost.predict(X_test)\n",
    "print('standard xgboost test acc: {}'.format((sum(predict_standard == np.array(y_test)))/len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 未使用min_split_loss进行预剪枝\n",
    "class XGBoostRegressionTree:\n",
    "    def __init__(self, max_depth=6, \n",
    "                 min_child_weight=1, reg_lambda=1, reg_alpha=0):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.reg_alpha = reg_alpha\n",
    "        \n",
    "        \n",
    "    def fit(self, x, grad, hess):\n",
    "        self.tree = self.build_tree(x, grad, hess, 0)\n",
    "    \n",
    "    \n",
    "    def build_tree(self, x, grad, hess, depth):\n",
    "        \n",
    "        # 当前节点是叶子节点\n",
    "        if depth >= self.max_depth:\n",
    "            return self.get_leaf_value(grad, hess)\n",
    "        \n",
    "        best_feature_index, best_threshold = self.choose_best_feature_to_split(x, grad, hess)\n",
    "        \n",
    "        # 当前节点是叶子节点\n",
    "        if best_feature_index is None or best_threshold is None:\n",
    "            return self.get_leaf_value(grad, hess)\n",
    "                \n",
    "        left_mask = x[:, best_feature_index] <= best_threshold\n",
    "        right_mask = x[:, best_feature_index] > best_threshold\n",
    "        \n",
    "        tree = {}\n",
    "        \n",
    "        tree[(best_feature_index, best_threshold, '<=')] = self.build_tree(x[left_mask], grad[left_mask], hess[left_mask], depth + 1)\n",
    "        tree[(best_feature_index, best_threshold, '>')] = self.build_tree(x[right_mask], grad[right_mask], hess[right_mask], depth + 1)\n",
    "        \n",
    "        return tree\n",
    "    \n",
    "    \n",
    "    def get_leaf_value(self, grad, hess):\n",
    "        G = np.sum(grad)\n",
    "        H = np.sum(hess)\n",
    "        \n",
    "        return -G/(H + self.reg_lambda)\n",
    "\n",
    "        \n",
    "    def predict_value(self, x):\n",
    "        tree = self.tree\n",
    "        \n",
    "        while type(tree).__name__ == 'dict':\n",
    "            \n",
    "            for key in tree.keys():\n",
    "                \n",
    "                if key[2] == '<=':\n",
    "                    key1 = key\n",
    "                elif key[2] == '>':\n",
    "                    key2 = key\n",
    "            \n",
    "            feature_index = key1[0]\n",
    "            threshold = key1[1]\n",
    "\n",
    "            \n",
    "            if x[feature_index] <= threshold:\n",
    "                tree = tree[key1]\n",
    "            elif x[feature_index] > threshold:\n",
    "                tree = tree[key2]\n",
    "        \n",
    "        if type(tree).__name__ != 'dict':\n",
    "            return tree\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_value(x) for x in X])\n",
    "    \n",
    "    \n",
    "    def calculate_gain(self, grad, hess, mask=None):\n",
    "        if mask is None:\n",
    "            G = np.sum(grad)\n",
    "            G *= G\n",
    "            H = np.sum(hess)\n",
    "        else:\n",
    "            G = np.sum(grad[mask])\n",
    "            G *= G\n",
    "            H = np.sum(hess[mask])\n",
    "        return G/(H + self.reg_lambda)\n",
    "    \n",
    "    \n",
    "    def choose_best_feature_to_split(self, x, grad, hess):\n",
    "        n_features = x.shape[1]\n",
    "        \n",
    "        total_gain = self.calculate_gain(grad, hess)\n",
    "        \n",
    "        best_gain = -sys.maxsize\n",
    "        best_feature_index = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        for feature_index in range(n_features):\n",
    "            values = np.array(list(set(x[:, feature_index])))\n",
    "            \n",
    "            for value in values:\n",
    "                left_mask = x[:, feature_index]<=value\n",
    "                right_mask = x[:, feature_index]>value\n",
    "                \n",
    "                left_gain = self.calculate_gain(grad, hess, left_mask)\n",
    "                right_gain = self.calculate_gain(grad, hess, right_mask)\n",
    "                \n",
    "                gain = (left_gain + right_gain - total_gain)/2 - self.reg_alpha\n",
    "                \n",
    "                if gain < 0 or self.not_valid_split(hess, left_mask) or self.not_valid_split(hess, right_mask):\n",
    "                    continue\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature_index = feature_index\n",
    "                    best_threshold = value\n",
    "               \n",
    "        # todo        \n",
    "        if best_threshold is None or best_feature_index is None:\n",
    "            pass\n",
    "        \n",
    "        return best_feature_index, best_threshold\n",
    "    \n",
    "    \n",
    "    def not_valid_split(self, hess, mask):\n",
    "        if np.sum(hess[mask]) < self.min_child_weight:\n",
    "            return True\n",
    "        \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 未使用col_sample_ratio和row_sample_ratio 因为和随机森林的思想类似，很容易理解\n",
    "class XGBoost:\n",
    "    def __init__(self, n_estimators, n_classes, learning_rate=0.3, row_sample_ratio=1, col_sample_ratio=1,\n",
    "                 reg_lambda=1, reg_alpha=0, max_depth=6):\n",
    "        self.row_sample_ratio = row_sample_ratio\n",
    "        self.col_sample_ratio = col_sample_ratio\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.reg_alpha = reg_alpha\n",
    "        self.max_depth = max_depth\n",
    "        self.n_estimators = n_estimators\n",
    "        self.n_classes = n_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.trees = {}\n",
    "    \n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        \n",
    "        y = np.array([self.to_one_hot(_y, self.n_classes) for _y in y])\n",
    "        \n",
    "        y_predict = np.full(y.shape, 0.5)\n",
    "        \n",
    "        for i in tqdm(range(self.n_estimators)):\n",
    "            self.trees[i] = {}\n",
    "            \n",
    "            grad = np.array([self.grad(y[j], y_predict[j]) for j in range(len(y))])\n",
    "            hess = np.array([self.hess(y_predict[j]) for j in range(len(y_predict))])\n",
    "            \n",
    "            for c in range(self.n_classes):           \n",
    "                \n",
    "                tree = XGBoostRegressionTree(max_depth=self.max_depth, \n",
    "                                             min_child_weight=1,\n",
    "                                             reg_lambda=self.reg_lambda,\n",
    "                                             reg_alpha=self.reg_alpha)\n",
    "                \n",
    "                tree.fit(x, grad[:, c], hess[:, c])\n",
    "                \n",
    "                self.trees[i][c] = tree\n",
    "                \n",
    "                # update y_predict\n",
    "                # 这个 y_predict 就是 GBDT 中的 F\n",
    "                for j in range(len(x)):\n",
    "                    y_predict[j][c] += self.learning_rate * self.trees[i][c].predict_value(x[j])\n",
    "    \n",
    "    \n",
    "    def predict_value(self, x):\n",
    "        p = [0]*self.n_classes\n",
    "        for m in self.trees:\n",
    "            for k in range(self.n_classes):\n",
    "                p[k] += self.learning_rate * self.trees[m][k].predict_value(x)\n",
    "        \n",
    "        return np.argmax(self.softmax(p))\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_value(x) for x in X])\n",
    "    \n",
    "    \n",
    "    # get first order gradient for single sample\n",
    "    # https://zhuanlan.zhihu.com/p/149419189\n",
    "    # 参考上面这篇文章，求一阶导时，i不等于j 和 i等于j 时 两者的形式虽然不同，\n",
    "    # 但是真实标签是one-hot（只有一个1），所以恰好可以表示为 y_probability - y\n",
    "    # 或许也可以直接由sigmoid的导数直接强推过来\n",
    "    def grad(self, y, y_predict):\n",
    "        y_probability = self.softmax(y_predict)\n",
    "        return y_probability - y\n",
    "    \n",
    "    \n",
    "    # get second order gradient for single sample\n",
    "    # 注意 y_predict 和 y_probability 的关系，可以理解为softmax的过程在交叉熵损失函数里面\n",
    "    # 这样交叉熵损失函数对y_predict求导就要涉及到softmax求导\n",
    "    def hess(self, y_predict):\n",
    "        y_probability = self.softmax(y_predict)\n",
    "        return y_probability * (1 - y_probability)\n",
    "    \n",
    "    \n",
    "    # convert label y to one-hot vector for single sample\n",
    "    def to_one_hot(self, y, n_classes):\n",
    "        one_hot = np.zeros(n_classes)\n",
    "        one_hot[y] = 1\n",
    "        \n",
    "        return one_hot\n",
    "    \n",
    "    \n",
    "    # convert y_predict to softmax probability for single sample\n",
    "    def softmax(self, a):\n",
    "        c = np.max(a)\n",
    "        exp_a = np.exp(a - c)\n",
    "        sum_exp_a = np.sum(exp_a)\n",
    "        p = exp_a / sum_exp_a\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training custom xgboost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:39<00:00, 19.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish training... time cost: 39.445207834243774s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "custom_xgboost = XGBoost(n_estimators=2, learning_rate=0.1, n_classes=10, max_depth=3)\n",
    "print('start training custom xgboost...')\n",
    "start = time.time()\n",
    "custom_xgboost.fit(X[:100], y[:100])\n",
    "end = time.time()\n",
    "print('finish training... time cost: {}s'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_custom = custom_xgboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom xgboost test acc: 0.4024\n"
     ]
    }
   ],
   "source": [
    "print('custom xgboost test acc: {}'.format((sum(predict_custom == np.array(y_test)))/len(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
