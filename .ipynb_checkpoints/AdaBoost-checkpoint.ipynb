{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import Counter\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    data = loadmat(\"data/mnist_all.mat\")\n",
    "\n",
    "    # print(data.keys())\n",
    "\n",
    "    train_data = pd.DataFrame()\n",
    "    test_data = pd.DataFrame()\n",
    "\n",
    "    for i in range(10):\n",
    "        temp_df = pd.DataFrame(data[\"train\" + str(i)])\n",
    "        temp_df['label'] = i\n",
    "        train_data = train_data.append(temp_df)\n",
    "        temp_df = pd.DataFrame(data[\"test\" + str(i)])\n",
    "        temp_df['label'] = i\n",
    "        test_data = test_data.append(temp_df)\n",
    "\n",
    "    train_data = shuffle(train_data)\n",
    "    test_data = shuffle(test_data)\n",
    "\n",
    "    train_labels = np.array(train_data['label'])\n",
    "    test_labels = np.array(test_data['label'])\n",
    "\n",
    "    train_data = train_data.drop('label', axis=1)\n",
    "    test_data = test_data.drop('label', axis=1)\n",
    "    \n",
    "    train_data = np.array(train_data) / 255\n",
    "    test_data = np.array(test_data) / 255\n",
    "    \n",
    "    pca = PCA(0.95)\n",
    "    pca.fit(train_data)\n",
    "    train_data = pca.transform(train_data)\n",
    "    test_data = pca.transform(test_data)\n",
    "\n",
    "    return train_data, test_data, train_labels, test_labels\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_data()\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意，经过PCA降维后，认为所有的特征都是连续值\n",
    "X = np.concatenate((X_train, X_valid), axis=0)\n",
    "y = np.concatenate((y_train, y_valid), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 154) (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME',\n",
       "                   base_estimator=DecisionTreeClassifier(max_depth=2),\n",
       "                   n_estimators=150)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "sklearn_adaboost = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=2),\n",
    "    n_estimators=150,\n",
    "    algorithm=\"SAMME\")\n",
    "sklearn_adaboost.fit(X[:1000], y[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sklearn = sklearn_adaboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn test acc: 0.7325\n"
     ]
    }
   ],
   "source": [
    "print('sklearn test acc: {}'.format((sum(predict_sklearn == np.array(y_test)))/len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CART_without_pruning:\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.build_tree(X, y)\n",
    "        \n",
    "          \n",
    "    def build_tree(self, X, y):\n",
    "        # 如果所有的X都属于一个y_i\n",
    "        temp_y = list(set(y))\n",
    "        if len(temp_y) == 1:\n",
    "            return temp_y[0]\n",
    " \n",
    "        # 如果没有可以选择的划分属性？\n",
    "        # 不会存在这种情况，因为连续属性可以重复用作划分属性\n",
    "        \n",
    "        best_feature_index, threshold, best_gini_index = self.choose_best_feature_to_split(X, y)\n",
    "        \n",
    "        if best_gini_index < self.epsilon:\n",
    "            return Counter(y).most_common(1)[0][0]\n",
    "        \n",
    "        tree = {}\n",
    "        x1, y1, x2, y2 = self.split_data(X, y, best_feature_index, threshold)\n",
    "        tree[(best_feature_index, threshold, '<=')] = self.build_tree(x1, y1)\n",
    "        tree[(best_feature_index, threshold, '>')] = self.build_tree(x2, y2)\n",
    "        \n",
    "        return tree\n",
    "    \n",
    "    \n",
    "    def split_data(self, X, y, best_feature_index, threshold):\n",
    "        \n",
    "        x1, x2, y1, y2 = [], [], [], []\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            if X[i][best_feature_index] <= threshold:\n",
    "                x1.append(X[i])\n",
    "                y1.append(y[i])\n",
    "            else:\n",
    "                x2.append(X[i])\n",
    "                y2.append(y[i])\n",
    "        \n",
    "        return np.array(x1), np.array(y1), np.array(x2), np.array(y2)\n",
    "    \n",
    "    \n",
    "    def predict(self, x):\n",
    "        tree = self.tree\n",
    "        \n",
    "        while type(tree).__name__ == 'dict':\n",
    "            \n",
    "            for key in tree.keys():\n",
    "                if key[2] == '<=':\n",
    "                    key1 = key\n",
    "                elif key[2] == '>':\n",
    "                    key2 = key\n",
    "                    \n",
    "                \n",
    "            feature_index = key1[0]\n",
    "            threshold = key1[1]\n",
    "            \n",
    "            if x[feature_index] <= threshold:\n",
    "                tree = tree[key1]\n",
    "            elif x[feature_index] > threshold:\n",
    "                tree = tree[key2]\n",
    "\n",
    "        \n",
    "        if type(tree).__name__ == 'int64' or type(tree).__name__ == 'int32':\n",
    "            return tree\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    def calculate_gini_index(self, feature_index, X, y):\n",
    "        values = []\n",
    "        for i in range(len(X)):\n",
    "            values.append(X[i][feature_index])\n",
    "        \n",
    "        values = list(set(values))\n",
    "        values.sort()\n",
    "        \n",
    "        best_gini_index = sys.maxsize\n",
    "        best_threshold = None\n",
    "        \n",
    "        for i in range(len(values) - 1):\n",
    "            threshold = (values[i] + values[i + 1])/2\n",
    "            \n",
    "            # D1和D2的作用是分别计算 <=threshold 和 >threshold 的X的数量\n",
    "            D1, D2 = 0, 0\n",
    "            \n",
    "            # d1和d2的作用是分别计算D1和D2中各类标签（0-9）的数量\n",
    "            d1, d2 = [0]*10, [0]*10\n",
    "            \n",
    "            for i in range(len(X)):\n",
    "                if X[i][feature_index] <= threshold:\n",
    "                    D1 += 1\n",
    "                    d1[y[i]] += 1\n",
    "                elif X[i][feature_index] > threshold:\n",
    "                    D2 += 1\n",
    "                    d2[y[i]] += 1\n",
    "            \n",
    "            # 下面计算gini index\n",
    "            gini_D1 = 0\n",
    "            gini_D2 = 0\n",
    "            \n",
    "            for i in range(10):\n",
    "                gini_D1 += math.pow(d1[i]/D1, 2)\n",
    "                gini_D2 += math.pow(d2[i]/D2, 2)\n",
    "                \n",
    "            gini_D1 = 1 - gini_D1\n",
    "            gini_D2 = 1 - gini_D2\n",
    "            \n",
    "            gini_index = gini_D1*D1/len(X) + gini_D2*D2/len(X)\n",
    "            \n",
    "            if gini_index < best_gini_index:\n",
    "                best_gini_index = gini_index\n",
    "                best_threshold = threshold\n",
    "                \n",
    "        return best_gini_index, best_threshold\n",
    "        \n",
    "        \n",
    "    # 根据西瓜书：\n",
    "    # 需注意的是，与离散属性不同，若当前结点划分属性为连续属性，该属性还可作为其后代结点的划分属性\n",
    "    # 因此不需要记录哪些属性（特征）已经使用过了\n",
    "    def choose_best_feature_to_split(self, X, y):\n",
    "        feature_num = X.shape[1]\n",
    "        \n",
    "        best_feature_index = -1\n",
    "        best_gini_index = sys.maxsize\n",
    "        best_feature_threshold = None\n",
    "        \n",
    "        for feature_index in range(feature_num):\n",
    "            gini_index, threshold = self.calculate_gini_index(feature_index, X, y)\n",
    "            \n",
    "            if gini_index < best_gini_index:\n",
    "                best_gini_index = gini_index\n",
    "                best_feature_index = feature_index\n",
    "                best_feature_threshold = threshold\n",
    "        \n",
    "        # 不会发生\n",
    "        if best_feature_index == -1:\n",
    "            pass\n",
    "        \n",
    "        return best_feature_index, best_feature_threshold, best_gini_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, n_estimators):\n",
    "        self.n_estimators = n_estimators\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.N = X.shape[0]\n",
    "        D = np.array([1/self.N]*self.N)\n",
    "        \n",
    "        self.estimators = []\n",
    "        self.alpha = []\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            estimator = CART_without_pruning(epsilon=0.001)\n",
    "            estimator.fit(X, y)\n",
    "            \n",
    "            G_m_x = []\n",
    "            e_m = 0\n",
    "            for i in range(self.N):\n",
    "                temp_y = estimator.predict(X[i])\n",
    "                \n",
    "                G_m_x.append(temp_y)\n",
    "                if temp_y != y[i]:\n",
    "                    e_m += D[i]\n",
    "            \n",
    "            alpha_m = np.log((1-e_m)/e_m)/2\n",
    "                \n",
    "            Z_m = 0\n",
    "            for i in range(self.N):\n",
    "                Z_m += D[i]*np.exp(-alpha_m*y[i]*G_m_x[i])\n",
    "            \n",
    "            _D = []\n",
    "            for i in range(self.N):\n",
    "                _D.append(np.exp(-alpha_m*y[i]*G_m_x[i])*D[i]/Z_m)\n",
    "            \n",
    "            D = copy.deepcopy(_D)\n",
    "            \n",
    "            self.alpha.append(alpha_m)\n",
    "            self.estimators.append(estimator)\n",
    "            \n",
    "    \n",
    "    def predict(self, x):\n",
    "        \n",
    "        y = 0\n",
    "        for i in range(self.n_estimators):\n",
    "            y += self.alpha[i] * self.estimators[i].predict(x)\n",
    "            \n",
    "        return np.sign(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_of_digit_i_and_j(x, y, digit_i, digit_j):\n",
    "    pos_sample_num = 0\n",
    "    neg_sample_num = 0\n",
    "    _data = []\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        if y[i] == digit_i and pos_sample_num < 100:\n",
    "            _data.append([x[i], 1])\n",
    "            pos_sample_num += 1\n",
    "\n",
    "        elif y[i] == digit_j and neg_sample_num < 100:\n",
    "            neg_sample_num += 1\n",
    "            _data.append([x[i], -1])\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    shuffle(_data)\n",
    "    data =  []\n",
    "    labels =[]\n",
    "    \n",
    "    for i in range(len(_data)):\n",
    "        data.append(_data[i][0])\n",
    "        labels.append(_data[i][1])\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = make_data_of_digit_i_and_j(X, y, 1, 2)\n",
    "test_data, test_labels = make_data_of_digit_i_and_j(X_test, y_test, 1, 2)\n",
    "train_data = np.array(train_data)\n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_adaboost = AdaBoost(n_estimators=3)\n",
    "custom_adaboost.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 22284.05it/s]\n"
     ]
    }
   ],
   "source": [
    "predict_list = []\n",
    "for i in tqdm(range(len(test_data))):\n",
    "    p = custom_adaboost.predict(test_data[i])\n",
    "    predict_list.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom test acc: 0.965\n"
     ]
    }
   ],
   "source": [
    "print('custom test acc: {}'.format((sum(predict_list == np.array(test_labels)))/len(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* train model of 0 and 1\n",
      "* train model of 0 and 2\n",
      "* train model of 0 and 3\n",
      "* train model of 0 and 4\n",
      "* train model of 0 and 5\n",
      "* train model of 0 and 6\n",
      "* train model of 0 and 7\n",
      "* train model of 0 and 8\n",
      "* train model of 0 and 9\n",
      "* train model of 1 and 2\n",
      "* train model of 1 and 3\n",
      "* train model of 1 and 4\n",
      "* train model of 1 and 5\n",
      "* train model of 1 and 6\n",
      "* train model of 1 and 7\n",
      "* train model of 1 and 8\n",
      "* train model of 1 and 9\n",
      "* train model of 2 and 3\n",
      "* train model of 2 and 4\n",
      "* train model of 2 and 5\n",
      "* train model of 2 and 6\n",
      "* train model of 2 and 7\n",
      "* train model of 2 and 8\n",
      "* train model of 2 and 9\n",
      "* train model of 3 and 4\n",
      "* train model of 3 and 5\n",
      "* train model of 3 and 6\n",
      "* train model of 3 and 7\n",
      "* train model of 3 and 8\n",
      "* train model of 3 and 9\n",
      "* train model of 4 and 5\n",
      "* train model of 4 and 6\n",
      "* train model of 4 and 7\n",
      "* train model of 4 and 8\n",
      "* train model of 4 and 9\n",
      "* train model of 5 and 6\n",
      "* train model of 5 and 7\n",
      "* train model of 5 and 8\n",
      "* train model of 5 and 9\n",
      "* train model of 6 and 7\n",
      "* train model of 6 and 8\n",
      "* train model of 6 and 9\n",
      "* train model of 7 and 8\n",
      "* train model of 7 and 9\n",
      "* train model of 8 and 9\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(i+1, 10):\n",
    "        \n",
    "        print('* train model of {} and {}'.format(i, j))\n",
    "        \n",
    "        model = AdaBoost(n_estimators=3)\n",
    "    \n",
    "        train_data, train_labels = make_data_of_digit_i_and_j(X_train, y_train, i, j)\n",
    "      \n",
    "        train_data = np.array(train_data)\n",
    "        \n",
    "        model.fit(train_data, train_labels)\n",
    "        \n",
    "        models[str(i) + str(j)] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10000/10000 [00:07<00:00, 1320.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc on test data of custom model: 0.7216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "correct_list = []\n",
    "error_list = []\n",
    "for i in tqdm(range(len(X_test))):\n",
    "    prob_list = []\n",
    "    for key in models.keys():\n",
    "        p = models[key].predict(X_test[i])\n",
    "        \n",
    "        py = None\n",
    "        if p == 1:\n",
    "            py = int(key[0])\n",
    "        elif p == -1:\n",
    "            py = int(key[1])\n",
    "        \n",
    "        prob_list.append(py)\n",
    "    \n",
    "    p = Counter(prob_list).most_common(1)[0][0]\n",
    "    \n",
    "    if p == y_test[i]:\n",
    "        correct += 1\n",
    "        correct_list.append(p)\n",
    "    \n",
    "    else:\n",
    "        error_list.append([p, y_test[i]])\n",
    "print('acc on test data of custom model: {}'.format(correct/len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
